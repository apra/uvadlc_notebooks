{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Performant Deep Learning - Multi GPU Programming\n",
    "\n",
    "**Filled notebook:** \n",
    "[![View on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/DL2/template/TemplateNotebook.ipynb) \n",
    "**Pre-trained models:** \n",
    "[![View files on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/saved_models/tree/main/DL2/template/)   \n",
    "**Recordings:** \n",
    "[![YouTube - Part N](https://img.shields.io/static/v1.svg?logo=youtube&label=YouTube&message=Part%20N&color=red)](https://youtu.be/waVZDFR-06U)    \n",
    "**Authors:**\n",
    "Your name here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TODOs:__\n",
    "\n",
    "* Update the links for the filled notebook (both github and collab) to your new notebook\n",
    "* Update the link for the saved models\n",
    "* Update the link for the YouTube recording if you have any. If you want to upload one to the UvA DLC YouTube account, you can contact Phillip.\n",
    "* Fill in the author names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Using multiple GPUs is a central part in scaling models to large datasets and obtain state of the art performance. \n",
    "\n",
    "We have seen that, to control multiple GPUs, we need to understand the concepts of distributed computing. The core problem in distributed computing is the communication between nodes, which requires synchronization. Luckily, we are equipped with very limited communication tools, that minimize the chance that problems arise (the specifics are outside the scope of this course, to get more insight into the possible issues, look into [concurrent programming](https://en.wikipedia.org/wiki/Concurrent_computing), race conditions, deadlocks, resource starvation, semaphores and barriers, and the book Operating Systems Internals and Design Principles)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tutorial content**. To better understand the primitives of communication in a distributed environment, we will begin by looking at some very basic exercises where simple operations are performed. Then, we will look at a more realistic scenario, the computation of the loss in a one-layer classifier (*more* realistic, but still *very* simple). Finally, we will learn how to run full-scale training on multiple GPUs and multiple nodes using PyTorch Lightning.\n",
    "\n",
    "#### Running the code\n",
    "The code in these cells is not meant to be run with this notebook. Instead, the files provided should be used in an environment where multiple GPUs are available. This step is not required (all the outputs and explanation of the code are available here), but highly encouraged, as getting familiar with these concepts, especially the more simple primitives, will help when more cryptic errors start appearing in big projects.\n",
    "\n",
    "Running the scripts can be done, for example, on the GPU partition of the LISA cluster ([General Knowledge on how to use the cluster](https://servicedesk.surfsara.nl/wiki/display/WIKI/SURFsara+Knowledge+Base)). After getting access using `ssh` (use WSL on Windows), we can setup the conda environment, by using the `module` package to load the correct anaconda version and then creating the environment based on the `environment.yml` file. It is also possible to install Anaconda on the login node, but this is not advised, as it can create dependency issues for you future projects, but for a simple experimentation it is feasible.\n",
    "\n",
    "To upload the code, the `rsync` command can be used.\n",
    "\n",
    "The main code to run is the following:\n",
    "```\n",
    "srun  -p gpu_shared -n 1 --ntasks-per-node 1 --gpus 2 --cpus-per-task 2 -t 1:00:00 --pty /bin/bash\n",
    "```\n",
    "\n",
    "where with `-p gpu_shared` we ask for the shared partition where there are GPUs available (other gpu partitions available are listed [here](https://servicedesk.surfsara.nl/wiki/display/WIKI/Lisa+usage+and+accounting)), then, we specify that we will be running only 1 task in this node, we want 2 GPUs and we use 2 CPUs as well, for 1 hour. The run consists of executing the command `/bin/bash` which starts a bash console on the node that we have been assigned. This allows for input of the necessary commands. \n",
    "\n",
    "Once inside, we can activate the correct anaconda environment and start running the scripts. We need to make sure that both GPUs are exposed to the script, with the following syntax:\n",
    "\n",
    "```\n",
    "CUDA_VISIBLE_DEVICES=0,1 python my_script.py\n",
    "```\n",
    "\n",
    "For these examples we will make use of the straightforward interface provided by [PyTorch](https://pytorch.org/docs/stable/distributed.html), a good summary is available at the documentation page, where all the details of the functions are shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some useful utilities\n",
    "\n",
    "The following code will help in the running of the experiments, with some plotting functions and setup of the distributed environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def rank_print(text: str):\n",
    "    \"\"\"\n",
    "    Prints a statement with an indication of what node rank is sending it\n",
    "    \"\"\"\n",
    "    rank = dist.get_rank()\n",
    "    # Keep the print statement as a one-liner to guarantee that\n",
    "    # one single process prints all the lines\n",
    "    print(f\"Rank: {rank}, {text}.\")\n",
    "\n",
    "\n",
    "def disk(\n",
    "    matrix: torch.Tensor,\n",
    "    center: tuple[int, int] = (1, 1),\n",
    "    radius: int = 1,\n",
    "    value: float = 1.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Places a disk with a certain radius and center in a matrix. The value given to the disk must be defined.\n",
    "    Something like this:\n",
    "    0 0 0 0 0\n",
    "    0 0 1 0 0\n",
    "    0 1 1 1 0\n",
    "    0 0 1 0 0\n",
    "    0 0 0 0 0\n",
    "\n",
    "    Arguments:\n",
    "     - matrix: the matrix where to place the shape.\n",
    "     - center: a tuple indicating the center of the disk\n",
    "     - radius: the radius of the disk in pixels\n",
    "     - value: the value to write where the disk is placed\n",
    "    \"\"\"\n",
    "    device = matrix.get_device()\n",
    "    shape = matrix.shape\n",
    "\n",
    "    # genereate the grid for the support points\n",
    "    # centered at the position indicated by position\n",
    "    grid = [slice(-x0, dim - x0) for x0, dim in zip(center, shape)]\n",
    "    x_coords, y_coords = np.mgrid[grid]\n",
    "    mask = torch.tensor(\n",
    "        ((x_coords / radius) ** 2 + (y_coords / radius) ** 2 <= 1), device=device\n",
    "    )\n",
    "    matrix = matrix * (~mask) + mask * value\n",
    "\n",
    "    return matrix, mask\n",
    "\n",
    "\n",
    "def square(\n",
    "    matrix: torch.tensor,\n",
    "    topleft: tuple[int, int] = (0, 0),\n",
    "    length: int = 1,\n",
    "    value: float = 1.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Places a square starting from the given top-left position and having given side length.\n",
    "    The value given to the disk must be defined.\n",
    "    Something like this:\n",
    "    0 0 0 0 0\n",
    "    0 1 1 1 0\n",
    "    0 1 1 1 0\n",
    "    0 1 1 1 0\n",
    "    0 0 0 0 0\n",
    "\n",
    "    Arguments:\n",
    "     - matrix: the matrix where to place the shape.\n",
    "     - topleft: a tuple indicating the top-left-most vertex of the square\n",
    "     - length: the side length of the square\n",
    "     - value: the value to write where the square is placed\n",
    "    \"\"\"\n",
    "    device = matrix.get_device()\n",
    "    shape = matrix.shape\n",
    "    grid = [slice(-x0, dim - x0) for x0, dim in zip(topleft, shape)]\n",
    "    x_coords, y_coords = np.mgrid[grid]\n",
    "    mask = torch.tensor(\n",
    "        (\n",
    "            (x_coords <= length)\n",
    "            & (x_coords >= 0)\n",
    "            & (y_coords >= 0)\n",
    "            & (y_coords <= length)\n",
    "        ),\n",
    "        device=device,\n",
    "    )\n",
    "    matrix = matrix * (~mask) + mask * value\n",
    "\n",
    "    return matrix, mask\n",
    "\n",
    "\n",
    "def plot_matrix(\n",
    "    matrix: torch.Tensor,\n",
    "    rank: int,\n",
    "    title: str = \"Matrix\",\n",
    "    name: str = \"image\",\n",
    "    folder: Optional[str] = None,\n",
    "    storefig: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Helper function to plot the images more easily. Can store them or visualize them right away.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.imshow(matrix.cpu(), cmap=\"tab20\", vmin=0, vmax=19)\n",
    "    plt.axis(\"off\")\n",
    "    if folder:\n",
    "        folder = Path(folder)\n",
    "        folder.mkdir(exist_ok=True, parents=True)\n",
    "    else:\n",
    "        folder = Path(\".\")\n",
    "\n",
    "    if storefig:\n",
    "        plt.savefig(folder / Path(f\"rank_{rank}_{name}.png\"))\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When starting the distributed environment, we need to decide a backend between `gloo`, `nccl` and `mpi`. The support for these libraries needs to be already available. The `nccl` backend should be already available from a GPU installation of PyTorch (CUDA Toolkit is required). On a Windows environment, only `gloo` works, but we will be running these scripts on a Unix environment.\n",
    "\n",
    "The second fundamental aspect is how the information is shared between nodes. The method we choose is through a shared file, that is accessible from all the GPUs. It is important to remember that access to this file should be quick for all nodes, so on LISA we will put it in the `scratch` folder.\n",
    "\n",
    "The other two parameters are the `rank` and `world_size`. The rank refers to the identifier for the current device, while the world size is the number of devices available for computation.\n",
    "\n",
    "When setting up the distributed environment, the correct GPU device should be selected. For simplicity, we select the GPU that has ID corresponding to the rank, but this is not necessary. \n",
    "\n",
    "Computation nodes could reside in different nodes, when this happens, using a shared fil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_distrib(rank: int, world_size: int, init_method: str=\"file:///scratch/sharedfile\", backend: str=\"nccl\"):\n",
    "    # select the correct device for this process\n",
    "    torch.cuda.set_device(rank)\n",
    "    \n",
    "    # initialize the processing group\n",
    "    torch.distributed.init_process_group(\n",
    "        backend=backend, world_size=world_size, init_method=init_method, rank=rank\n",
    "    )\n",
    "    \n",
    "    # return the current device\n",
    "    return torch.device(f\"cuda:{rank}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a model on multiple GPUs is a clear example that we will keep in mind throughout this tutorial to contextualize how to make use of the available primitives of communication in distributed computing.\n",
    "\n",
    "Initially, we want to give the current weights of the model to every GPU that we are using. To do so, we will **broadcast** the necessary tensors.\n",
    "\n",
    "Then, each GPU will collect a subset of the full batch, lets say only 64 out of 256 samples, from memory and perform a forward pass of the model. At the end, we need to compute the loss over the entire batch of 256 samples, but no GPU can fit all of these. Here, the **reduction** primitive comes to the resque. The tensors that reside in different GPUs are collected and an operation is performed that will *reduce* the tensors to a single one. This allows for the result of the operation to still fit in memory. We may want to keep thisresult in a single GPU (using **reduce**) or send it to all of them (using **all_reduce**).\n",
    "\n",
    "The operations that we can perform are determined by the backend that we are currently using. When using `nccl`, the list of available operations is the following:\n",
    " - SUM\n",
    " - AVG (only version2.10 or higher)\n",
    " - PRODUCT\n",
    " - MIN\n",
    " - MAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Reduce\n",
    "\n",
    "As we can see from the illustration, the **all reduce** primitive performs an operation between the tensors present in each GPU and replaces them with the result of the operation. the file to run is `all_reduce.py`.\n",
    "\n",
    "<img src=\"images/allreduce.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import setup_distrib, disk, square, rank_print, plot_matrix\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch\n",
    "\n",
    "# operation performed by the reduce\n",
    "OPERATION = dist.ReduceOp.MAX\n",
    "\n",
    "\n",
    "def main_process(rank:int, world_size:int=2):\n",
    "    device = setup_distrib(rank, world_size)\n",
    "    rank_print(\"test\")\n",
    "    image = torch.zeros((11,11), device=device)\n",
    "    \n",
    "    if rank == 0:\n",
    "        rank_image, rank_mask = disk(image, (3,3), 2, rank+1, device)\n",
    "    elif rank == 1:\n",
    "        rank_image, rank_mask = square(image, (3,3), 2, rank+1, device)\n",
    "    \n",
    "    plot_matrix(rank_image, rank, title=f\"Rank {rank} Before All Reduce\", name=\"before_all_reduce\", folder=\"all_reduce\")\n",
    "    \n",
    "    # The main operation\n",
    "    dist.all_reduce(rank_image, op=OPERATION)\n",
    "    plot_matrix(rank_image, rank, title=f\"Rank {rank} After All Reduce Operation: {OPERATION}\", name=\"after_all_reduce\", folder=\"all_reduce\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mp.spawn(main_process, nprocs=2, args=())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this operation is illustrated in the figure below. The operation is performed between the tensors stored in the different devices and the result is spread across all devices.\n",
    "\n",
    "<img src=\"images/slide_all_Reduce.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce\n",
    "\n",
    "As we can see from the illustration, the **reduce** primitive performs an operation between the tensors present in each GPU and sends the result only to the root rank. In Pytorch, we can define the destination rank.\n",
    "\n",
    "<img src=\"images/reduce.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import setup_distrib, disk, square, rank_print, plot_matrix\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch\n",
    "DESTINATION_RANK = 0\n",
    "OPERATION = dist.ReduceOp.MAX\n",
    "\n",
    "def main_process(rank, world_size=2):\n",
    "    device = setup_distrib(rank, world_size)\n",
    "    \n",
    "    image = torch.zeros((11,11), device=device)\n",
    "    \n",
    "    if rank == 0:\n",
    "        rank_image, rank_mask = disk(image, (4,5), 2, rank+1, device)\n",
    "    elif rank == 1:\n",
    "        rank_image, rank_mask = square(image, (3,3), 2, rank+1, device)\n",
    "    \n",
    "    plot_matrix(rank_image, rank, title=f\"Rank {rank} Before Reduce\", name=\"before_reduce\", folder=\"reduce\")\n",
    "    \n",
    "    # The main operation\n",
    "    dist.reduce(rank_image, dst=DESTINATION_RANK, op=OPERATION)\n",
    "    plot_matrix(rank_image, rank, title=f\"Rank {rank} After Reduce Operation: {OPERATION}\", name=\"after_reduce\", folder=\"reduce\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mp.spawn(main_process, nprocs=2, args=())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are shown below. We can see how only the rank 0, the one we selected, has the result of the operation. This helps in reducing the processing time, if the operation is executed in an asynchronous way, all other GPUs can keep processing while the root one is receiving the result.\n",
    "\n",
    "<img src=\"images/slide_reduce.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast\n",
    "\n",
    "The **broadcast** operation is fundamental, as it allows to send (broadcast) data from one GPU to all others in the network.\n",
    "\n",
    "<img src=\"images/broadcast.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import setup_distrib, disk, square, rank_print, plot_matrix\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch\n",
    "SOURCE_RANK = 0\n",
    "OPERATION = dist.ReduceOp.MAX\n",
    "\n",
    "def main_process(rank, world_size=2):\n",
    "    device = setup_distrib(rank, world_size)\n",
    "    \n",
    "    image = torch.zeros((11,11), device=device)\n",
    "    \n",
    "    if rank == 0:\n",
    "        rank_image, rank_mask = disk(image, (4,5), 2, rank+1, device)\n",
    "    elif rank == 1:\n",
    "        rank_image, rank_mask = square(image, (3,3), 2, rank+1, device)\n",
    "    \n",
    "    plot_matrix(rank_image, rank, name=\"before_broadcast\", folder=\"broadcast\")\n",
    "    \n",
    "    # The main operation\n",
    "    dist.broadcast(rank_image, src=SOURCE_RANK)\n",
    "    plot_matrix(rank_image, rank, name=\"after_broadcast\", folder=\"broadcast\")\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mp.spawn(main_process, nprocs=2, args=())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this illustration we see how, the rank 1 GPU gets the correct image after the broadcast is performed.\n",
    "\n",
    "<img src=\"images/slide_broadcast.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Gather\n",
    "\n",
    "The **all gather** operation allows for all GPUs to have access to all the data processed by the others. This can be expecially useful when different operations need to be performed by each GPU, after a common operation has been performed on each subset of the data. It is important to note that the entirety of the data needs to fit in a single GPU, so here the bottleneck won't be the memory, instead, it will be the processing speed.\n",
    "\n",
    "<img src=\"images/allgather.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import setup_distrib, disk, square, rank_print, plot_matrix\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch\n",
    "def main_process(rank, world_size=2):\n",
    "    device = setup_distrib(rank, world_size)\n",
    "    \n",
    "    image = torch.zeros((11,11), device=device)\n",
    "    \n",
    "    rank_images = []\n",
    "    \n",
    "    if rank == 0:\n",
    "        rank_images.append(disk(image, (4,5), 2, rank+1, device)[0])\n",
    "    elif rank == 1:\n",
    "        rank_images.append(disk(image, (7,6), 2, rank+1, device)[0])\n",
    "        \n",
    "    output_tensors = []\n",
    "    for _ in range(world_size):\n",
    "        output_tensors.append(torch.zeros_like(image, device=device))\n",
    "    \n",
    "    plot_matrix(output_tensors[0], rank, title=f\"Rank {rank}\", name=\"before_gather_0\", folder=\"all_gather\")\n",
    "    plot_matrix(output_tensors[1], rank, title=f\"\", name=\"before_gather_1\", folder=\"all_gather\")\n",
    "    \n",
    "    dist.all_gather(output_tensors, rank_images[0])\n",
    "    plot_matrix(output_tensors[0], rank, title=f\"Rank {rank}\", name=\"after_gather_0\", folder=\"all_gather\")\n",
    "    plot_matrix(output_tensors[1], rank, title=f\"\", name=\"after_gather_1\", folder=\"all_gather\")\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mp.spawn(main_process, nprocs=2, args=())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the result of the **all_gather**. All GPUs now have access to the data that was initially only present in some of them.\n",
    "\n",
    "<img src=\"images/slide_all_gather.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Scatter\n",
    "\n",
    "With **reduce scatter** we can perform an operation on just a subset of the whole data and have each GPU have the partial results.\n",
    "\n",
    "<img src=\"images/reducescatter.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import setup_distrib, disk, square, rank_print, plot_matrix\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch\n",
    "OPERATION = dist.ReduceOp.MAX\n",
    "def main_process(rank, world_size=2):\n",
    "    device = setup_distrib(rank, world_size)\n",
    "    \n",
    "    image = torch.zeros((11,11), device=device)\n",
    "    \n",
    "    input_tensors = []\n",
    "    \n",
    "    if rank == 0:\n",
    "        input_tensors.append(disk(image, (4,5), 2, rank+1, device)[0])\n",
    "        input_tensors.append(square(image, (5,5), 3, rank+1, device)[0])\n",
    "    elif rank == 1:\n",
    "        input_tensors.append(disk(image, (7,6), 2, rank+1, device)[0])\n",
    "        input_tensors.append(square(image, (0,2), 4, rank+1, device)[0])\n",
    "        \n",
    "    output = torch.zeros_like(image, device=device)\n",
    "    \n",
    "    plot_matrix(input_tensors[0], rank, title=f\"Rank {rank}\", name=\"before_reduce_scatter_0\", folder=\"reduce_scatter\")\n",
    "    plot_matrix(input_tensors[1], rank, title=f\"\", name=\"before_reduce_scatter_1\", folder=\"reduce_scatter\")\n",
    "    plot_matrix(output, rank, title=f\"\", name=\"before_reduce_scatter\", folder=\"reduce_scatter\")\n",
    "    \n",
    "    dist.reduce_scatter(output, input_tensors, op=OPERATION)\n",
    "    plot_matrix(output, rank, title=f\"\", name=\"after_reduce_scatter\", folder=\"reduce_scatter\")\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mp.spawn(main_process, nprocs=2, args=())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this figure we see that only one image is available at the end, allowing for the operation to be performed across GPUs while keeping the overall final memory footprint low.\n",
    "<img src=\"images/slide_reduce_scatter.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from utils import rank_print\n",
    "\n",
    "DSET_FOLDER = \"/var/scratch/spapa/\"\n",
    "def main_process(rank, world_size=2):\n",
    "    print(f\"Process for rank: {rank} has been spawned\")\n",
    "    \n",
    "    # Set the correct device for this process rank\n",
    "    torch.cuda.set_device(rank)\n",
    "    device = torch.device(f\"cuda:{rank}\")\n",
    "    \n",
    "    torch.distributed.init_process_group(\n",
    "        backend='nccl', world_size=world_size, init_method='file:///var/scratch/spapa/sharedfile', rank=rank\n",
    "    )\n",
    "    \n",
    "    # Load the dataset in all processes\n",
    "    if rank == 0:\n",
    "        dset = torchvision.datasets.CIFAR10(DSET_FOLDER, download=True)\n",
    "    # Make sure download has finished\n",
    "    dist.barrier()\n",
    "\n",
    "    dset = torchvision.datasets.CIFAR10(DSET_FOLDER)\n",
    "    \n",
    "    \n",
    "    \n",
    "    input_size = 3*32*32  # [channel size, height, width]\n",
    "    batch_size = 256\n",
    "    num_classes = 10\n",
    "    if dist.get_rank() == 0:\n",
    "        weights = torch.rand((input_size, num_classes), device=device)\n",
    "    else:\n",
    "        weights = torch.zeros((input_size, num_classes), device=device)\n",
    "\n",
    "    # Distribute weights to all GPUs\n",
    "    handle = dist.broadcast(tensor = weights, src=0, async_op=True)\n",
    "    handle.wait()\n",
    "    rank_print(f\"Weights received.\")\n",
    "    \n",
    "    # Flattened images\n",
    "    cur_input = torch.zeros((batch_size, input_size), device=device)\n",
    "    # One-Hot encoded target\n",
    "    cur_target = torch.zeros((batch_size, num_classes), device=device)    \n",
    "    for i in range(batch_size):\n",
    "        rank_print(rank, f\"Loading image {i+world_size*rank} into GPU...\")\n",
    "        image, target = dset[i+world_size*rank]\n",
    "        cur_input[i] = transforms.ToTensor()(image).flatten()\n",
    "        cur_target[i, target] = 1.\n",
    "    \n",
    "    # Compute the linear part of the layer\n",
    "    output = torch.matmul(cur_input, weights)\n",
    "    rank_print(f\"\\nComputed output: {output}, Size: {output.size()}\")\n",
    "        \n",
    "    # Define the activation function of the output layer\n",
    "    logsoftm = torch.nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    # Apply activation function to output layer\n",
    "    output = logsoftm(output)\n",
    "    rank_print(rank, f\"\\nLog-Softmaxed output: {output}, Size: {output.size()}\")\n",
    "    \n",
    "    loss = output.sum(dim=1).mean()\n",
    "    rank_print(rank, f\"Loss: {loss}, Size: {loss.size()}\")\n",
    "    \n",
    "    handle = dist.reduce(tensor=loss, dst=0, op=dist.ReduceOp.SUM, async_op=True)\n",
    "    handle.wait()\n",
    "    \n",
    "    rank_print(f\"Loss: {loss}\")\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    mp.spawn(main_process, nprocs=2, args=())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Give a conclusion and summary of the notebook. Give a retroview: what have the students learned from this notebook, what is there to further explore in this topic, anything critical to keep in mind?\n",
    "\n",
    "### References\n",
    "\n",
    "Give a list of references, especially the papers that introduce the methods you implemented in this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
